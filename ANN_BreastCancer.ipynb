{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vidhamjot Kaur\n",
    "# C0909093\n",
    "\n",
    "**https://github.com/VidhamjotKaur/Breast-Cancer-Data-Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment Guide: Breast Cancer Data Analysis and Streamlit App\n",
    "Step 1: Project Setup\n",
    "1. Create a Project Directory:\n",
    "o Create a new directory for your project in VS Code.\n",
    "o Initialize a Git repository.\n",
    "2. Set Up a Virtual Environment:\n",
    "o Create and activate a virtual environment for the project.\n",
    "Step 2: Dataset Acquisition and Preparation\n",
    "1. Download the Dataset:\n",
    "o Download the Breast Cancer dataset from a reliable source like the UCI Machine\n",
    "Learning Repository, Kaggle or get the dataset from sklearn.\n",
    "2. Data Preparation:\n",
    "o Write a Python script to load and preprocess the dataset, ensuring it is ready for\n",
    "analysis.\n",
    "Step 3: Feature Selection\n",
    "1. Feature Selection Technique:\n",
    "o Implement feature selection using methods like SelectKBest from\n",
    "sklearn.feature_selection.\n",
    "Step 4: Grid Search CV for Model Tuning\n",
    "1. Grid Search Cross-Validation:\n",
    "o Provide a template or guide for setting up Grid Search CV to optimize the\n",
    "parameters of an ANN model (MLPClassifier from sklearn.neural_network).\n",
    "Step 5: Implementing an Artificial Neural Network (ANN) Model\n",
    "1. ANN Model Creation:\n",
    "o Outline the steps to create an ANN model.\n",
    "o Train and evaluate the model using the breast cancer dataset.\n",
    "Step 6: Building a Streamlit App Locally\n",
    "1. Streamlit code:\n",
    "o Use Streamlit as a tool for building interactive web apps with Python.\n",
    "2. Developing the Streamlit App:\n",
    "o Create a basic Streamlit app that allows users to interact with the breast cancer\n",
    "dataset and view model predictions.\n",
    "o Integrate model predictions, and user interaction within the Streamlit app.\n",
    "Step 7: Deployment and Version Control\n",
    "1. GitHub Repository Setup:\n",
    "o Setting up a GitHub repository for their project. Give the link in the comment\n",
    "section.\n",
    "o Commit their code regularly and push changes to GitHub.\n",
    "2. Submission Requirements:\n",
    "o Specify the deliverables, such as the Python scripts, Streamlit app code, and a\n",
    "README.md file documenting the project.\n",
    "Additional Tips\n",
    "• Documentation and Comments: Emphasize the importance of clear documentation and\n",
    "comments in the code to explain each step and rationale.\n",
    "• Encourage Exploration: Encourage students to explore different feature selection\n",
    "techniques, model architectures, and hyperparameter configurations beyond the basic\n",
    "requirements.\n",
    "By following these steps, students can gain hands-on experience in data preprocessing, model\n",
    "development, and interactive web application creation using Streamlit, enhancing their\n",
    "understanding of machine learning concepts and practical skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install joblib\n",
    "!pip install streamlit\n",
    "!pip install numpy\n",
    "!pip install warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each feature:\n",
      " mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "dtype: int64\n",
      "Selected Features: ['mean radius', 'mean perimeter', 'mean area', 'mean concavity', 'mean concave points', 'worst radius', 'worst perimeter', 'worst area', 'worst concavity', 'worst concave points']\n",
      "Selected features saved to 'selected_features.pkl'.\n",
      "Scaler saved to 'scaler.pkl'.\n",
      "Fitting 3 folds for each of 240 candidates, totalling 720 fits\n",
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "Best cross-validation accuracy: 0.9604246543511096\n",
      "Confusion Matrix:\n",
      " [[42  1]\n",
      " [ 2 69]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97        43\n",
      "           1       0.99      0.97      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "Trained model saved to 'mlp_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load the breast cancer dataset from scikit-learn.\n",
    "\n",
    "    Returns:\n",
    "        X (pd.DataFrame): Feature data.\n",
    "        y (pd.Series): Target labels.\n",
    "    \"\"\"\n",
    "    data = load_breast_cancer()\n",
    "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "    y = pd.Series(data.target, name='target')\n",
    "    return X, y\n",
    "\n",
    "def check_missing_values(X):\n",
    "    \"\"\"\n",
    "    Check and print the number of missing values in each feature.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature data.\n",
    "    \"\"\"\n",
    "    missing = X.isnull().sum()\n",
    "    print(\"Missing values in each feature:\\n\", missing)\n",
    "\n",
    "def select_features(X, y, k=10):\n",
    "    \"\"\"\n",
    "    Select the top k features based on ANOVA F-value.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature data.\n",
    "        y (pd.Series): Target labels.\n",
    "        k (int): Number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "        X_selected_df (pd.DataFrame): DataFrame containing the selected features.\n",
    "        selected_features (List[str]): List of selected feature names.\n",
    "    \"\"\"\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    print(\"Selected Features:\", selected_features.tolist())\n",
    "    \n",
    "    # Save the selected features\n",
    "    joblib.dump(selected_features, 'selected_features.pkl')\n",
    "    print(\"Selected features saved to 'selected_features.pkl'.\")\n",
    "    \n",
    "    X_selected_df = pd.DataFrame(X_selected, columns=selected_features)\n",
    "    return X_selected_df, selected_features\n",
    "\n",
    "def standardize_features(X, selected_features):\n",
    "    \"\"\"\n",
    "    Standardize the selected features using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): DataFrame containing selected features.\n",
    "        selected_features (List[str]): List of selected feature names.\n",
    "\n",
    "    Returns:\n",
    "        X_scaled_df (pd.DataFrame): Standardized feature data.\n",
    "        scaler (StandardScaler): Fitted scaler object.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=selected_features)\n",
    "    \n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "    print(\"Scaler saved to 'scaler.pkl'.\")\n",
    "    \n",
    "    return X_scaled_df, scaler\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature data.\n",
    "        y (pd.Series): Target labels.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: Split data.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def perform_grid_search(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform an expanded Grid Search to find the best hyperparameters for MLPClassifier.\n",
    "\n",
    "    This function defines a moderately extensive parameter grid and uses GridSearchCV to\n",
    "    search for the optimal combination of hyperparameters. The search includes various\n",
    "    configurations for hidden layer sizes, activation functions, solvers, regularization\n",
    "    parameters, and learning rates.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training feature data.\n",
    "        y_train (pd.Series): Training target labels.\n",
    "\n",
    "    Returns:\n",
    "        clf (GridSearchCV): Fitted GridSearchCV object with the best found parameters.\n",
    "    \"\"\"\n",
    "    # Define an expanded yet reasonable parameter grid for Grid Search\n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': [\n",
    "            (50,), (100,), (100, 50), (100, 100), (150, 100)\n",
    "        ],\n",
    "        'activation': ['tanh', 'relu', 'logistic'],\n",
    "        'solver': ['adam', 'lbfgs'],\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.05],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "    }\n",
    "\n",
    "    # Initialize the MLPClassifier with a random state for reproducibility\n",
    "    mlp = MLPClassifier(\n",
    "        max_iter=500,  # Increased from 100 to 500 for better convergence\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Initialize GridSearchCV with the expanded parameter grid\n",
    "    clf = GridSearchCV(\n",
    "        estimator=mlp,\n",
    "        param_grid=parameter_space,\n",
    "        n_jobs=-1,          # Utilize all available CPU cores\n",
    "        cv=3,               # 5-fold cross-validation\n",
    "        verbose=2,          # Verbosity level for detailed logs\n",
    "        scoring='accuracy'  # Evaluation metric\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV to the training data\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Display the best parameters found by Grid Search\n",
    "    print('Best parameters found:\\n', clf.best_params_)\n",
    "    print('Best cross-validation accuracy:', clf.best_score_)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the test set and print metrics.\n",
    "\n",
    "    Args:\n",
    "        model (MLPClassifier): Trained MLPClassifier model.\n",
    "        X_test (pd.DataFrame): Testing feature data.\n",
    "        y_test (pd.Series): Testing target labels.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "def save_model(model, filename='mlp_model.pkl'):\n",
    "    \"\"\"\n",
    "    Save the trained model to a file.\n",
    "\n",
    "    Args:\n",
    "        model (MLPClassifier): Trained MLPClassifier model.\n",
    "        filename (str): Filename to save the model.\n",
    "    \"\"\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"Trained model saved to '{filename}'.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the machine learning pipeline:\n",
    "    - Load data\n",
    "    - Check for missing values\n",
    "    - Select features\n",
    "    - Standardize features\n",
    "    - Split data\n",
    "    - Perform Grid Search\n",
    "    - Train and evaluate the model\n",
    "    - Save the trained model and preprocessing objects\n",
    "    \"\"\"\n",
    "    # Optionally suppress convergence warnings (not recommended)\n",
    "    # warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    # Load dataset\n",
    "    X, y = load_dataset()\n",
    "    \n",
    "    # Check for missing values\n",
    "    check_missing_values(X)\n",
    "    \n",
    "    # Feature Selection\n",
    "    X_selected_df, selected_features = select_features(X, y, k=10)\n",
    "    \n",
    "    # Standardize Features (only selected features)\n",
    "    X_scaled_df, scaler = standardize_features(X_selected_df, selected_features)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = split_data(X_scaled_df, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Perform Grid Search to find the best MLPClassifier parameters\n",
    "    clf = perform_grid_search(X_train, y_train)\n",
    "    \n",
    "    # Train the final model with best parameters\n",
    "    best_mlp = clf.best_estimator_\n",
    "    best_mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(best_mlp, X_test, y_test)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(best_mlp, 'mlp_model.pkl')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
